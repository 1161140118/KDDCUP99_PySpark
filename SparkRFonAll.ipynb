{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化spark环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession # SparkSession 是Spark 2.0版本的新入口\n",
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"duration\",IntegerType(),False),\n",
    "StructField(\"protocol_type\",StringType(),False),\n",
    "StructField(\"service\",StringType(),False),\n",
    "StructField(\"flag\",StringType(),False),\n",
    "StructField(\"src_bytes\",IntegerType(),False),\n",
    "StructField(\"dst_bytes\",IntegerType(),False),\n",
    "StructField(\"land\",IntegerType(),False),\n",
    "StructField(\"wrong_fragment\",IntegerType(),False),\n",
    "StructField(\"urgent\",IntegerType(),False),\n",
    "StructField(\"hot\",IntegerType(),False),\n",
    "StructField(\"num_failed_logins\",IntegerType(),False),\n",
    "StructField(\"logged_in\",IntegerType(),False),\n",
    "StructField(\"num_compromised\",IntegerType(),False),\n",
    "StructField(\"root_shell\",IntegerType(),False),\n",
    "StructField(\"su_attempted\",IntegerType(),False),\n",
    "StructField(\"num_root\",IntegerType(),False),\n",
    "StructField(\"num_file_creations\",IntegerType(),False),\n",
    "StructField(\"num_shells\",IntegerType(),False),\n",
    "StructField(\"num_access_files\",IntegerType(),False),\n",
    "StructField(\"num_outbound_cmds\",IntegerType(),False),\n",
    "StructField(\"is_host_login\",IntegerType(),False),\n",
    "StructField(\"is_guest_login\",IntegerType(),False),\n",
    "StructField(\"count\",IntegerType(),False),\n",
    "StructField(\"srv_count\",IntegerType(),False),\n",
    "StructField(\"serror_rate\",FloatType(),False),\n",
    "StructField(\"srv_serror_rate\",FloatType(),False),\n",
    "StructField(\"rerror_rate\",FloatType(),False),\n",
    "StructField(\"srv_rerror_rate\",FloatType(),False),\n",
    "StructField(\"same_srv_rate\",FloatType(),False),\n",
    "StructField(\"diff_srv_rate\",FloatType(),False),\n",
    "StructField(\"srv_diff_host_rate\",FloatType(),False),\n",
    "StructField(\"dst_host_count\",IntegerType(),False),\n",
    "StructField(\"dst_host_srv_count\",IntegerType(),False),\n",
    "StructField(\"dst_host_same_srv_rate\",FloatType(),False),\n",
    "StructField(\"dst_host_diff_srv_rate\",FloatType(),False),\n",
    "StructField(\"dst_host_same_src_port_rate\",FloatType(),False),\n",
    "StructField(\"dst_host_srv_diff_host_rate\",FloatType(),False),\n",
    "StructField(\"dst_host_serror_rate\",FloatType(),False),\n",
    "StructField(\"dst_host_srv_serror_rate\",FloatType(),False),\n",
    "StructField(\"dst_host_rerror_rate\",FloatType(),False),\n",
    "StructField(\"dst_host_srv_rerror_rate\",FloatType(),False),\n",
    "StructField(\"label\",StringType(),False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从本地读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: integer (nullable = true)\n",
      " |-- dst_bytes: integer (nullable = true)\n",
      " |-- land: integer (nullable = true)\n",
      " |-- wrong_fragment: integer (nullable = true)\n",
      " |-- urgent: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- num_failed_logins: integer (nullable = true)\n",
      " |-- logged_in: integer (nullable = true)\n",
      " |-- num_compromised: integer (nullable = true)\n",
      " |-- root_shell: integer (nullable = true)\n",
      " |-- su_attempted: integer (nullable = true)\n",
      " |-- num_root: integer (nullable = true)\n",
      " |-- num_file_creations: integer (nullable = true)\n",
      " |-- num_shells: integer (nullable = true)\n",
      " |-- num_access_files: integer (nullable = true)\n",
      " |-- num_outbound_cmds: integer (nullable = true)\n",
      " |-- is_host_login: integer (nullable = true)\n",
      " |-- is_guest_login: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- srv_count: integer (nullable = true)\n",
      " |-- serror_rate: float (nullable = true)\n",
      " |-- srv_serror_rate: float (nullable = true)\n",
      " |-- rerror_rate: float (nullable = true)\n",
      " |-- srv_rerror_rate: float (nullable = true)\n",
      " |-- same_srv_rate: float (nullable = true)\n",
      " |-- diff_srv_rate: float (nullable = true)\n",
      " |-- srv_diff_host_rate: float (nullable = true)\n",
      " |-- dst_host_count: integer (nullable = true)\n",
      " |-- dst_host_srv_count: integer (nullable = true)\n",
      " |-- dst_host_same_srv_rate: float (nullable = true)\n",
      " |-- dst_host_diff_srv_rate: float (nullable = true)\n",
      " |-- dst_host_same_src_port_rate: float (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate: float (nullable = true)\n",
      " |-- dst_host_serror_rate: float (nullable = true)\n",
      " |-- dst_host_srv_serror_rate: float (nullable = true)\n",
      " |-- dst_host_rerror_rate: float (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate: float (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从csv中读取\n",
    "dataset = spark.read.csv('Data/kddcup.data.gz', header=None,schema=schema)\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并稀疏特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mapper_mergeSparseFeatureInService\n",
    "import pyspark.sql.functions as F\n",
    "# 用户自定义函数\n",
    "udf_mergeSparseFeatureInService = F.UserDefinedFunction(mapper_mergeSparseFeatureInService,StringType())\n",
    "\n",
    "dataset = dataset.select(\n",
    "    *[udf_mergeSparseFeatureInService(column).alias('service') if column == 'service' else column for column in dataset.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分攻击类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import mapper_attack2majorindex\n",
    "\n",
    "udf_mapper_attack2majorindex = F.UserDefinedFunction(mapper_attack2majorindex,IntegerType())\n",
    "\n",
    "dataset = dataset.select(\n",
    "    *[udf_mapper_attack2majorindex(column).alias('label') if column == 'label' else column for column in dataset.columns]\n",
    ")\n",
    "dataset.select('label').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类编码 Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-------+-------------+----+----------+-----+\n",
      "|protocol_type|protocol_type_index|service|service_index|flag|flag_index|label|\n",
      "+-------------+-------------------+-------+-------------+----+----------+-----+\n",
      "|          tcp|                1.0|   http|          2.0|  SF|       0.0|    0|\n",
      "|          tcp|                1.0|   http|          2.0|  SF|       0.0|    0|\n",
      "|          tcp|                1.0|   http|          2.0|  SF|       0.0|    0|\n",
      "|          tcp|                1.0|   http|          2.0|  SF|       0.0|    0|\n",
      "|          tcp|                1.0|   http|          2.0|  SF|       0.0|    0|\n",
      "+-------------+-------------------+-------+-------------+----+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "fearture_string = ['protocol_type', 'service', 'flag']\n",
    "pipeline_stringindex = Pipeline(stages=[\n",
    "    StringIndexer(inputCol=c, outputCol='{}_index'.format(c),handleInvalid='keep')\n",
    "    for c in fearture_string\n",
    "])\n",
    "\n",
    "pipeline_stringindex_fitted = pipeline_stringindex.fit(dataset=dataset)\n",
    "\n",
    "dataset_indexed = pipeline_stringindex_fitted.transform(dataset=dataset)\n",
    "\n",
    "dataset_indexed.select('protocol_type','protocol_type_index','service','service_index','flag','flag_index','label').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造训练集 [features][label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresToVec(dataset_indexed):\n",
    "    # 构造特征向量\n",
    "    features=[dataset_indexed.columns[0]]+dataset_indexed.columns[4:]\n",
    "    features.remove('label')\n",
    "    feature_vec = VectorAssembler(inputCols=features,outputCol='feature_vec').transform(dataset_indexed)\n",
    "    # 组合数据集： 特征向量，标签\n",
    "    return feature_vec.select('feature_vec','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_indexed = featuresToVec(dataset_indexed)\n",
    "# 划分训练集，测试集\n",
    "train_set,test_set = dataset_indexed.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    featuresCol='feature_vec', # 特征\n",
    "    labelCol='label', #标签\n",
    "    impurity='gini', # 考虑到样本分布和计算效率\n",
    "    maxDepth=8, # 最大深度\n",
    "    maxBins=72, # 根据样本训练集确定\n",
    "    numTrees=200, # 树个数\n",
    "    subsamplingRate = 0.5, # 默认值为1，用于训练每棵树的数据占总训练集比例，可用于加速\n",
    "    seed=5, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trains in 4353.546 seconds\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "t0 = time()\n",
    "rf_model = rf.fit(train_set)\n",
    "tt = time() -t0\n",
    "print(\"Trains in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存训练好的机器学习模型\n",
    "rf_model.save('SparkRFonAll/rf_SparkRF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载本地机器学习模型\n",
    "rf_model.load('SparkRFonAll/rf_SparkRF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perdict in 0.113 seconds\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "t0 = time()\n",
    "test_result = rf_model.transform(test_set)\n",
    "tt = time() -t0\n",
    "print(\"Perdict in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉验证打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy                       0.999515 \t 69.876000\n",
      "f1                             0.999506 \t 99.630000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction')\n",
    "score={}\n",
    "# for metric in ['accuracy','f1','weightedPrecision','weightedRecall']:\n",
    "for metric in ['accuracy','f1']:\n",
    "    t0 = time()\n",
    "    score[metric] = evaluator.evaluate(dataset=test_result,params={evaluator.metricName:metric})\n",
    "    tt = time() -t0\n",
    "    print(\"%-*s %f \\t %f seconds\"%( 30 , metric, score[metric], round(tt,3)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证集打分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据导入与处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = spark.read.csv('Data/corrected.gz', header=None,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并稀疏特征\n",
    "corrected = corrected.select(\n",
    "    *[udf_mergeSparseFeatureInService(column).alias('service') if column == 'service' else column for column in corrected.columns]\n",
    ")\n",
    "# 攻击类型分类\n",
    "corrected = corrected.select(\n",
    "    *[udf_mapper_attack2majorindex(column).alias('label') if column == 'label' else column for column in corrected.columns]\n",
    ")\n",
    "# 分类编码\n",
    "corrected_indexed = pipeline_stringindex_fitted.transform(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分 features vector 和 label\n",
    "corrected_set = featuresToVec(corrected_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perdict in 0.029 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "corrected_result = rf_model.transform(corrected_set)\n",
    "tt = time() -t0\n",
    "print(\"Perdict in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy                       0.923210 \t 131.599000 seconds\n",
      "f1                             0.903593 \t 176.263000 seconds\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction')\n",
    "score={}\n",
    "# for metric in ['accuracy','weightedPrecision','weightedRecall','f1']:\n",
    "for metric in ['accuracy','f1']:\n",
    "    t0 = time()\n",
    "    score[metric] = evaluator.evaluate(dataset=corrected_result,params={evaluator.metricName:metric})\n",
    "    tt = time() -t0\n",
    "    print(\"%-*s %f \\t %f seconds\"%( 30 , metric, score[metric], round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils as ut\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-46cee215ba7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "imp.load_module(utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.calculateRecall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DF column to list\n",
    "def getListFromDF(df,colname):\n",
    "    tmp = df.select(colname).toPandas()\n",
    "    return tmp.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接根据测试结果DF 绘制混淆矩阵,并输出评分\n",
    "def drawFromDF(dataset):\n",
    "    true = getListFromDF(dataset,'label')\n",
    "    pred = getListFromDF(dataset,'prediction')\n",
    "    cm = drawConfusionMatrix(true,pred,[0,1,2,3,4])\n",
    "    cm.show()\n",
    "    preAndRec(cm=cm,labels=[0,1,2,3,4])\n",
    "    return cm,true,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drawConfusionMatrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a6755bb9e430>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdrawFromDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-ef70034e736e>\u001b[0m in \u001b[0;36mdrawFromDF\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetListFromDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetListFromDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrawConfusionMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpreAndRec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'drawConfusionMatrix' is not defined"
     ]
    }
   ],
   "source": [
    "test_cm,test_true,test_pred = drawFromDF(dataset=test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_cm,correct_true,correct_pred = drawFromDF(dataset=correct_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
